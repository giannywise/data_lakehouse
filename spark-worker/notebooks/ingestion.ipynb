{
 "cells": [
  {
   "cell_type": "code",
   "id": "9e457e59-1d86-43b7-af69-400d5df17f28",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "#spark.stop()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d16610b4-8b04-419b-94f2-2bcde4e5bfd2",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lakehouse\") \\\n",
    "    .config(\"spark.delta.columnMapping.mode\", \"name\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\",\"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\",\"s3a://hive/\") \\\n",
    "    .config(\"spark.sql.hive.metastore.version\",\"3.1.3\")\\\n",
    "    .config(\"spark.sql.hive.metastore.jars\",\"path\") \\\n",
    "    .config(\"spark.sql.hive.metastore.jars.path\",\"file:///opt/spark/hive/jars/*\") \\\n",
    "    .config(\"spark.sql.legacy.charVarcharAsString\", True)\\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\\\n",
    "    .config(\"hive.metastore.warehouse.dir\",\"s3a://hive/\") \\\n",
    "    .config(\"spark.hive.metastore.schema.verification\",\"false\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Minimierung des LOGS\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "log4jLogger = spark._jvm.org.apache.log4j\n",
    "logger = log4jLogger.LogManager.getLogger(\"LOGGER\")\n",
    "logger.setLevel(log4jLogger.Level.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"data/\"\n",
    "ignore_file = \".DS_Store\"\n",
    "# Liste, um die Namen zu sammeln\n",
    "names = []\n",
    "# For-Loop, um durch den Verzeichnisinhalt zu iterieren und nur die Namen zu erfassen\n",
    "for entry in os.scandir(directory):\n",
    "    # Überspringe die Datei, wenn sie den ignorierten Namen hat\n",
    "    if entry.name == ignore_file:\n",
    "        continue\n",
    "    names.append((entry.name,))\n",
    "# Spark DataFrame mit einer Spalte \"name\" erstellen\n",
    "df_file_list = spark.createDataFrame(names, [\"name\"])\n",
    "\n",
    "# DataFrame anzeigen\n",
    "df_file_list.show(truncate=False)"
   ],
   "id": "8bfb3e468df53cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DB_NAME = \"fidus_hbs\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME}\")\n",
    "spark.sql(f'USE {DB_NAME}')\n",
    "\n",
    "# Verwende einen relativen Pfad und konvertiere in einen absoluten Pfad:\n",
    "relative_path = f\"data/{DB_NAME}.db\"\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "jdbc_url = f\"jdbc:sqlite:{absolute_path}\""
   ],
   "id": "fe98894a-5802-4e82-9d6d-76d39d2b5594",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Beispiel: Abrufen aller Tabellennamen aus der SQLite-Datenbank\n",
    "df_tables = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", \"(SELECT name FROM sqlite_master WHERE type='table') as tables\") \\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "    .load()\n",
    "df_tables.show(truncate=False)\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS fidus\")\n"
   ],
   "id": "7f77c2b7-84c2-4403-b68b-520f45485ad2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_names = [row[\"name\"] for row in df_tables.collect()]\n",
    "for table in table_names:\n",
    "    print(table)"
   ],
   "id": "fc9af24202418b4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"data/\"\n",
    "ignore_file = \".DS_Store\"\n",
    "# Liste, um die Namen zu sammeln\n",
    "names = []\n",
    "# For-Loop, um durch den Verzeichnisinhalt zu iterieren und nur die Namen zu erfassen\n",
    "for entry in os.scandir(directory):\n",
    "    # Überspringe die Datei, wenn sie den ignorierten Namen hat\n",
    "    if entry.name == ignore_file:\n",
    "        continue\n",
    "    names.append((entry.name,))\n",
    "# Spark DataFrame mit einer Spalte \"name\" erstellen\n",
    "df = spark.createDataFrame(names, [\"name\"])\n",
    "\n",
    "# DataFrame anzeigen\n",
    "df.show(truncate=False)"
   ],
   "id": "60bd30e4757be989",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8df5b878-3c20-41bd-978a-c4c0a9256508",
   "metadata": {},
   "source": [
    "table_names = [row[\"name\"] for row in df_tables.collect()]\n",
    "for TABLE_NAME in table_names:\n",
    "    print(f\"{TABLE_NAME} Started!\")\n",
    "    df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", TABLE_NAME).option(\"driver\", \"org.sqlite.JDBC\").load()\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {TABLE_NAME} cascade\") # Löscht den MetaStore Eintrag\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", f's3a://bronze/fidus/{DB_NAME}/{TABLE_NAME}/').saveAsTable(TABLE_NAME)\n",
    "    print(f\"{TABLE_NAME} Done!\")\n",
    "\n",
    "print(\"SUCCESS!\")\n",
    "#df.show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "624c437d-7658-46cb-af11-4402fc64ce18",
   "metadata": {},
   "source": [
    "# Schreibt Delta Table und erstellt einen externen Table\n",
    "\n",
    "#spark.sql(f\"DROP DATABASE IF EXISTS {TABLE_NAME} cascade\") # Löscht den MetaStore Eintrag\n",
    "#df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", f's3a://bronze/fidus/fidus_and/{TABLE_NAME}/').saveAsTable(TABLE_NAME)\n",
    "#print(\"done\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f4c0ec4-5038-49d3-b0fc-3c79419c18b1",
   "metadata": {},
   "source": "#spark.sql(f\"select * from {TABLE_NAME}\").show(5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44007dd6-87bf-4756-8b08-233169d6f34f",
   "metadata": {},
   "source": [
    "#df2 = spark.read.format(\"delta\").load(\"s3a://bronze/fidus/fidus_and\")\n",
    "# Zeige die Daten an\n",
    "#df2.show(truncate=False)\n",
    "#df2.count()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "261ff7be-d24d-4e07-96e4-435a2591af5e",
   "metadata": {},
   "source": [
    "#dt = DeltaTable.forName(spark, table_name)\n",
    "#dt.toDF().show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73184fd0-2bbc-4b9a-9b56-d0d43e89c0e2",
   "metadata": {},
   "source": "#spark.sql(\"DESCRIBE TABLe EXTENDED adrver\").show(100,truncate=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48c19f7f-8147-4d72-9dbd-e842957a29c6",
   "metadata": {},
   "source": "#spark.sql(\"SHOW TABLES\").show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fca2926-088c-4555-8b02-e73c448db7da",
   "metadata": {},
   "source": [
    "#spark.sql(\"drop database if exists begrun cascade\")\n",
    "#spark.sql(\"DROP database IF EXISTS fidus_sue CASCADE\")\n",
    "spark.sql(\"SHOW databases\").show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b4535c4-59d5-436b-8ba5-5c211560f218",
   "metadata": {},
   "source": [
    "spark.sql(\"create schema delta_test\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8468e8d0-4f85-4a52-b172-9c5bea49f45f",
   "metadata": {},
   "source": "spark.catalog.listTables(\"default\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36fed533-fcc0-4263-9f99-1f6feccd7281",
   "metadata": {},
   "source": [
    "spark.catalog.listFunctions(\"hive_table\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b12c4fc9-b83f-4318-9af4-c79cd82dffdd",
   "metadata": {},
   "source": [
    "directory = \"data/\"\n",
    "ignore_file = \".DS_Store\"\n",
    "\n",
    "# Liste, um die Namen zu sammeln\n",
    "names = []\n",
    "\n",
    "# For-Loop, um durch den Verzeichnisinhalt zu iterieren und nur die Namen zu erfassen\n",
    "for entry in os.scandir(directory):\n",
    "    # Ignoriere die Datei \".DS_Store\"\n",
    "    if entry.name == ignore_file:\n",
    "        continue\n",
    "    # Überprüfen, ob der Dateiname mit \".db\" endet\n",
    "    if entry.name.endswith(\".db\"):\n",
    "        # Endung entfernen\n",
    "        name_without_extension = os.path.splitext(entry.name)[0]\n",
    "        names.append((name_without_extension,))\n",
    "    else:\n",
    "        # Anderenfalls den Originalnamen verwenden\n",
    "        names.append((entry.name,))\n",
    "\n",
    "df_file_list = spark.createDataFrame(names, [\"name\"])\n",
    "\n",
    "for row in df_file_list.collect():\n",
    "    DB_NAME = row[\"name\"]\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME}\")\n",
    "    spark.sql(f'USE {DB_NAME}')\n",
    "\n",
    "    # Verwende einen relativen Pfad und konvertiere in einen absoluten Pfad:\n",
    "    relative_path = f\"data/{DB_NAME}.db\"\n",
    "    absolute_path = os.path.abspath(relative_path)\n",
    "    jdbc_url = f\"jdbc:sqlite:{absolute_path}\"\n",
    "\n",
    "    table_names = [row[\"name\"] for row in df_tables.collect()]\n",
    "    #print(f\"{DB_NAME} Started!\")\n",
    "    for TABLE_NAME in table_names:\n",
    "        #print(f\"{TABLE_NAME} Started!\")\n",
    "        df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", TABLE_NAME).option(\"driver\", \"org.sqlite.JDBC\").load()\n",
    "        spark.sql(f\"DROP DATABASE IF EXISTS {TABLE_NAME} cascade\") # Löscht den MetaStore Eintrag\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"path\", f's3a://bronze/fidus/{DB_NAME}/{TABLE_NAME}/').saveAsTable(TABLE_NAME)\n",
    "        #print(f\"{TABLE_NAME} Done!\")\n",
    "\n",
    "    print(f\"{DB_NAME} SUCCESS!\")\n",
    "print(\"ALL DONE!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T22:16:21.605054Z",
     "start_time": "2025-02-20T22:16:21.369451Z"
    }
   },
   "cell_type": "code",
   "source": "spark.sql(\"SHOW tables\").show()",
   "id": "4d87decffb7073ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|fidus_mhw| behdatei|      false|\n",
      "|fidus_mhw| praxarzt|      false|\n",
      "|fidus_mhw|   patdat|      false|\n",
      "|fidus_mhw|   adrver|      false|\n",
      "|fidus_mhw|   begrun|      false|\n",
      "|fidus_mhw| arbehdat|      false|\n",
      "|fidus_mhw| rechkopf|      false|\n",
      "|fidus_mhw|  rechpos|      false|\n",
      "|fidus_mhw| ekktexte|      false|\n",
      "|fidus_mhw| ckeytabs|      false|\n",
      "|fidus_mhw|  patinfo|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "177cec66-ae6b-465e-85de-fcec018973d0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
